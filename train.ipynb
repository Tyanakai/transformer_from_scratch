{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1cidhDHT9NvoNMk0SIyliKsh0uxiQTGa9",
      "authorship_tag": "ABX9TyMOv7uc8gvWXC3PpC+cAmbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyanakai/transformer_from_scratch/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# tokenizer output dict\n",
        "# tf dataset"
      ],
      "metadata": {
        "id": "wAAyuGxtB4CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-0qW-An2kE"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRRRh8zgwgHl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIYsmCsqwnBf"
      },
      "source": [
        "class Config:\n",
        "    train_file = \"date.txt\"\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7AVJ3RwIpG"
      },
      "source": [
        "DRIVE = \"/content/drive/MyDrive/portfolio/transformer_study\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4B1-B9zs_P"
      },
      "source": [
        "## preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JR7MuTFwyjN"
      },
      "source": [
        "with open(os.path.join(DRIVE, Config.train_file), mode=\"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uKVMOtrpxcNb",
        "outputId": "3238879f-701a-4f68-a84a-702ff11c7792"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'september 27, 1994           _1994-09-27\\nAugust 19, 2003              _2003-08-19\\n2/10/93           '"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvEP6SwxdcJ"
      },
      "source": [
        "text_x = []\n",
        "text_y = []\n",
        "\n",
        "for line in text.split(\"\\n\")[:-1]:\n",
        "    text_x.append(line[:-11].lower().lstrip())\n",
        "    text_y.append(line[-10:].lstrip())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApsiwTn-0GA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aff4c28f-5984-47d7-973d-c0ea3f33af98"
      },
      "source": [
        "text_x[-1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'thursday, november 20, 1980  '"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MzQAv_S0KSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0367f013-be81-4165-f83a-031f101e10eb"
      },
      "source": [
        "text_y[-1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1980-11-20'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "GvrE2a4y45A-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4daI9TvCra2l"
      },
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self, text_list):\n",
        "        self.text_list = text_list\n",
        "\n",
        "\n",
        "    def pad_text(self, text):\n",
        "        # 文末の空白をpad文字で埋める\n",
        "        last_char_idx = len(text.strip())\n",
        "        text = text[:last_char_idx] + \"＠\" * (len(text) - last_char_idx)\n",
        "        return text\n",
        "\n",
        "\n",
        "    def create_char_list(self):\n",
        "        # 入力文字列を文字に分解する\n",
        "        self.char_list = []\n",
        "        for text in self.text_list:\n",
        "            text = self.pad_text(text) # 文末の空白をpad文字で埋める\n",
        "            self.char_list.append(list(text)) # 文字に分解しリスト化\n",
        "\n",
        "    \n",
        "    def create_char_id_dict(self):\n",
        "        # 出現文字にidを対応させる\n",
        "        self.id_char_dict = dict()\n",
        "        self.char_id_dict = dict()\n",
        "        self.unique_char = np.unique(self.char_list)\n",
        "        \n",
        "        for id, c in enumerate(self.unique_char):\n",
        "            self.id_char_dict[id] = c\n",
        "            self.char_id_dict[c] = id\n",
        "\n",
        "\n",
        "    def attention_mask(self):\n",
        "        # attention_maskを作る\n",
        "        attention_mask = []\n",
        "        for line in self.char_list:\n",
        "            chars = np.array(line) # 文字リストをnp.array化\n",
        "            attention_mask.append((chars != \"＠\") * 1) # 文字が＠ではない場所が1となる\n",
        "        return np.array(attention_mask)\n",
        "\n",
        "\n",
        "    def tokenize(self):\n",
        "        # 文字列を文字に分解しリスト化\n",
        "        self.create_char_list()\n",
        "        \n",
        "        # 出現文字にidを対応させる\n",
        "        self.create_char_id_dict()\n",
        "\n",
        "        # 文字をidに変換する\n",
        "        token_list = []\n",
        "        for text in self.text_list:\n",
        "            token_list.append([self.char_id_dict[c] for c in text])\n",
        "\n",
        "        return np.array(token_list)\n",
        "\n",
        "\n",
        "    def detokenize(self, token_list):\n",
        "        for line in token_list:\n",
        "            char_list = [self.id_char_dict[t] for t in line]\n",
        "        return char_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_tokenizer = Tokenizer(text_x)\n",
        "encoder_token_list = encoder_tokenizer.tokenize()\n",
        "encoder_attention_mask = encoder_tokenizer.attention_mask()\n",
        "encoder_num_char = encoder_tokenizer.unique_char.shape[0]\n",
        "\n",
        "decoder_tokenizer = Tokenizer(text_y)\n",
        "decoder_token_list = decoder_tokenizer.tokenize()\n",
        "decoder_attention_mask = decoder_tokenizer.attention_mask()\n",
        "decoder_num_char = decoder_tokenizer.unique_char.shape[0]"
      ],
      "metadata": {
        "id": "U6gUE4EHDofE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(decoder_token_list).shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0dtPeO13RuB",
        "outputId": "fceca3e3-9e55-40d5-a62f-ee2dab34cd8a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.data.Dataset"
      ],
      "metadata": {
        "id": "O2l8kLi6_PX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(\n",
        "    encoder_input_ids, \n",
        "    encoder_attention_mask, \n",
        "    decoder_input_ids, \n",
        "    decoder_attention_mask,\n",
        "    \n",
        "    ):\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(\n",
        "        encoder_input_ids, \n",
        "        encoder_attention_mask, \n",
        "        decoder_input_ids, \n",
        "        decoder_attention_mask\n",
        "        )\n",
        "    if \n",
        "    ds = ds.shuffle(1024)\n",
        "    ds = ds.batch(32)\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "lY7xCIVs_SjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ixpknrx8_SUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "M3u_HxuWKhQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### layers"
      ],
      "metadata": {
        "id": "D_y8u17P5MP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class EncoderSelfAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    encoder側のself attention\n",
        "    Attributes:\n",
        "        weight_dim: 入力に積算する重みの次元 (int)\n",
        "        num_heads: multi head attentionのhead数 (int)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, weight_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.weight_dim = weight_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "\n",
        "    def split_transpose(self, x):\n",
        "        \"\"\"\n",
        "        xをheadの数に分割し、後の積のため転置する\n",
        "        Args:  \n",
        "            x: tensor (batch_size, max_length, weight_dim)\n",
        "        Returns: \n",
        "            x: tensor (batch_size, num_heads, max_length, weight_dim/num_heads)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, [x.shape[0], x.shape[1], self.num_heads, -1])\n",
        "        x = tf.transpose(x, perm=[0,2,1,3])\n",
        "        return x\n",
        "\n",
        "\n",
        "    def create_mask_for_pad(self, attention_mask1, attention_mask2):\n",
        "        \"\"\"\n",
        "        paddingの位置を無視する為のmaskを作る\n",
        "        Args: \n",
        "            attention_mask1: np.array (batch_size, max_length1)  padの位置 = 0\n",
        "            attention_mask2: np.array (batch_size, max_length2)  padの位置 = 0\n",
        "        Returns:\n",
        "            (batch_size, num_heads, max_length1, max_length2)  padの位置 = True\n",
        "        Note:\n",
        "            #1 　(batch_size, max_length1, max_length2)のmaskを作り\n",
        "            #2 　headの数だけrepeatし\n",
        "            #3 　0,1を反転させる\n",
        "        \"\"\"\n",
        "        p_mask = np.array([m1.reshape(-1, 1) * m2 for m1, m2 in zip(attention_mask1, \n",
        "                                                                    attention_mask2)])  #1\n",
        "        p_mask = np.repeat(p_mask[:,None,:,:], self.num_heads, axis=1)  #2\n",
        "        p_mask = 1 - p_mask  #3\n",
        "        return tf.cast(p_mask, tf.bool)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.wq = self.add_weight(\n",
        "            \"wq\", shape=[input_shape[-1], self.weight_dim])\n",
        "        self.wk = self.add_weight(\n",
        "            \"wk\", shape=[input_shape[-1], self.weight_dim])\n",
        "        self.wv = self.add_weight(\n",
        "            \"wv\", shape=[input_shape[-1], self.weight_dim])\n",
        "        self.wo = self.add_weight(\n",
        "            \"wo\", shape=[self.weight_dim, input_shape[-1]])\n",
        "        super().build(input_shape)\n",
        "        \n",
        "        \n",
        "    def call(self, input, attention_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: tensor (batch_size, max_length, hidden_dim)\n",
        "            attention_mask: np.array (batch_size, max_length) padの位置 = 0\n",
        "        \n",
        "        Returns:\n",
        "            tensor (batch_size, max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        q = tf.matmul(input, self.wq)\n",
        "        k = tf.matmul(input, self.wk)\n",
        "        v = tf.matmul(input, self.wv)\n",
        "\n",
        "        q = self.split_transpose(q)\n",
        "        k = self.split_transpose(k)\n",
        "        v = self.split_transpose(v)\n",
        "\n",
        "        p_mask = self.create_mask_for_pad(attention_mask, attention_mask)\n",
        "        mask = tf.cast(p_mask, tf.float32)\n",
        "\n",
        "        logit = tf.matmul(q, k, transpose_b=True)\n",
        "        logit += logit.dtype.min * mask   # set pad position to \"-inf\"\n",
        "\n",
        "        attention_weight = tf.nn.softmax(\n",
        "            logit / tf.sqrt(tf.cast(self.weight_dim, tf.float32)))\n",
        "        multi_context_vec = tf.matmul(attention_weight, v)\n",
        "        \n",
        "        multi_context_vec = tf.transpose(multi_context_vec, perm=[0,2,1,3])\n",
        "        concat_vec = tf.reshape(\n",
        "            multi_context_vec, \n",
        "            shape=[input.shape[0], input.shape[1], self.weight_dim]\n",
        "            )\n",
        "        encoded_vec = tf.matmul(concat_vec, self.wo)\n",
        "        return encoded_vec\n",
        "\n",
        "\n",
        "class DecoderSelfAttention(EncoderSelfAttention):\n",
        "    \"\"\"\n",
        "    decoder側のself attention\n",
        "    Attributes:\n",
        "        weight_dim: 入力に積算する重みの次元 (int)\n",
        "        num_heads: multi head attentionのhead数 (int)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, weight_dim, num_heads, **kwargs):\n",
        "        super().__init__(weight_dim, num_heads, **kwargs)\n",
        "\n",
        "\n",
        "    def create_mask_for_future_input(self, input):\n",
        "        \"\"\"\n",
        "        自身より未来のinputを参照しない為のmaskを作る\n",
        "        Args:\n",
        "            input: tensor (batch_size, num_heads, max_length, max_length)\n",
        "        Returns:\n",
        "            tensor (batch_size, num_heads, max_length, max_length) maskの位置 = True\n",
        "        Notes:\n",
        "            右上三角行列 - 対角行列　＝　未来時刻の値が1のマスク行列 (f-mask)\n",
        "            [[0, 1, 1, 1]\n",
        "            [0, 0, 1, 1]\n",
        "            [0, 0, 0, 1] \n",
        "            [0, 0, 0, 0]]\n",
        "        \"\"\"\n",
        "        ones = np.ones(input.shape)\n",
        "\n",
        "        # 右上三角行列 - 対角行列\n",
        "        f_mask = tf.linalg.band_part(ones, 0, -1) \\\n",
        "               - tf.linalg.band_part(ones, 0, 0)\n",
        "        return tf.cast(f_mask, tf.bool)\n",
        "        \n",
        "        \n",
        "    def call(self, input, attention_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: tensor (batch_size, max_length, hidden_dim)\n",
        "            attention_mask: np.array (batch_size, max_length) padの位置 = 0\n",
        "        \n",
        "        Returns:\n",
        "            tensor (batch_size, max_length, hidden_dim)\n",
        "        Notes:\n",
        "            future maskを適用する点でEncoderSelfAttentionのcallと異なる\n",
        "        \"\"\"\n",
        "        q = tf.matmul(input, self.wq)\n",
        "        k = tf.matmul(input, self.wk)\n",
        "        v = tf.matmul(input, self.wv)\n",
        "        \n",
        "        q = self.split_transpose(q)\n",
        "        k = self.split_transpose(k)\n",
        "        v = self.split_transpose(v)\n",
        "\n",
        "        logit = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        f_mask = self.create_mask_for_future_input(logit) # create future mask\n",
        "        p_mask = self.create_mask_for_pad(attention_mask, attention_mask)\n",
        "        mask = tf.cast(tf.logical_or(f_mask, p_mask), tf.float32)\n",
        "        \n",
        "        logit += logit.dtype.min * mask  # set future or pad position to \"-inf\"\n",
        "\n",
        "        attention_weight = tf.nn.softmax(\n",
        "            logit / tf.sqrt(tf.cast(self.weight_dim, tf.float32)))\n",
        "        multi_context_vec = tf.matmul(attention_weight, v)\n",
        "        \n",
        "        multi_context_vec = tf.transpose(multi_context_vec, perm=[0,2,1,3])\n",
        "        concat_vec = tf.reshape(\n",
        "            multi_context_vec, \n",
        "            shape=[input.shape[0], input.shape[1], self.weight_dim]\n",
        "            )\n",
        "        encoded_vec = tf.matmul(concat_vec, self.wo)\n",
        "        return encoded_vec\n",
        "\n",
        "\n",
        "class EncoderDecoderAttention(EncoderSelfAttention):\n",
        "    \"\"\"\n",
        "    decoder側のlayer\n",
        "    decoder側のself attentionの出力と共に、encoder側の出力も参照する\n",
        "    Attributes:\n",
        "        weight_dim: 入力に積算する重みの次元 (int)\n",
        "        num_heads: multi head attentionのhead数 (int)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight_dim, num_heads, **kwargs):\n",
        "        super().__init__(weight_dim, num_heads, **kwargs)\n",
        "        \n",
        "\n",
        "    def call(self, \n",
        "             decoder_input, \n",
        "             decoder_attention_mask, \n",
        "             encoder_output, \n",
        "             encoder_attention_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_input: tensor (batch_size, decoder_max_length, hidden_dim)\n",
        "            decoder_attention_mask: np.array (batch_size, decoder_max_length) padの位置 = 0\n",
        "            encoder_output: tensor (batch_size, encoder_max_length, hidden_dim)\n",
        "            encoder_attention_mask: np.array (batch_size, decoder_max_length) padの位置 = 0\n",
        "        Returns:\n",
        "            tensor (batch_size, decoder_max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        \n",
        "        q = tf.matmul(decoder_input, self.wq)\n",
        "        k = tf.matmul(encoder_output, self.wk)\n",
        "        v = tf.matmul(encoder_output, self.wv)\n",
        "\n",
        "        q = self.split_transpose(q)\n",
        "        k = self.split_transpose(k)\n",
        "        v = self.split_transpose(v)\n",
        "\n",
        "        p_mask = self.create_mask_for_pad(decoder_attention_mask, encoder_attention_mask)\n",
        "        mask = tf.cast(p_mask, tf.float32)\n",
        "\n",
        "        logit = tf.matmul(q, k, transpose_b=True)\n",
        "        logit += logit.dtype.min * mask   # set pad position to \"-inf\"\n",
        "\n",
        "        attention_weight = tf.nn.softmax(\n",
        "            logit / tf.sqrt(tf.cast(self.weight_dim, tf.float32)))\n",
        "        multi_context_vec = tf.matmul(attention_weight, v)\n",
        "        \n",
        "        multi_context_vec = tf.transpose(multi_context_vec, perm=[0,2,1,3])\n",
        "        concat_vec = tf.reshape(\n",
        "            multi_context_vec, \n",
        "            shape=[decoder_input.shape[0], decoder_input.shape[1], self.weight_dim]\n",
        "            )\n",
        "        encoded_vec = tf.matmul(concat_vec, self.wo)\n",
        "        return encoded_vec\n",
        "\n",
        "\n",
        "class LayerNormalizer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    文単位で正規化を行う\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.scale = self.add_weight(\n",
        "            \"scale\", initializer=tf.keras.initializers.Constant(1.))\n",
        "        self.bias = self.add_weight(\n",
        "            \"bias\", initializer=tf.keras.initializers.Constant(0.))\n",
        "        super().build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: tensor (batch_size, max_length, hidden_dim)\n",
        "        \n",
        "        Returns:\n",
        "            tensor (batch_size, max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        mean = tf.math.reduce_mean(input, axis=[1,2])[:, tf.newaxis, tf.newaxis]\n",
        "        std = tf.math.reduce_std(input, axis=[1,2])[:, tf.newaxis, tf.newaxis]\n",
        "        normalized = (input - mean) / (std + K.epsilon())\n",
        "        output = normalized * self.scale + self.bias\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForwardNeuralBlock(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    encoder, decoder両方で使用する全結合layer\n",
        "    Attributes:\n",
        "        hidden_dim: 全結合層の重みの次元\n",
        "        dropout_rate: dropout層のパラメータ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.filter_layer = tf.keras.layers.Dense(\n",
        "            hidden_dim*4, activation=\"relu\", use_bias=True, name=\"filter_layer\")\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            hidden_dim, use_bias=True, name=\"output_layer\")\n",
        "        \n",
        "      \n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: tensor (batch_size, max_length, hidden_dim)\n",
        "        Returns\n",
        "            tensor (batch_size, max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        x = self.filter_layer(input)\n",
        "        x = self.dropout(x)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    入力されたtokenベクトルに位置ベクトルを加算するlayer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    \n",
        "    def positional_vec(self, pos, embd_dim):\n",
        "        \"\"\"\n",
        "        位置ベクトルを計算する\n",
        "        \n",
        "        Args:\n",
        "            pos: 文におけるtokenの位置\n",
        "            embd_dim: tokenベクトルの次元\n",
        "        Returns:\n",
        "            pos_v: np.array (None, pos, embd_dim)\n",
        "                   ブロードキャストの為batch_sizeの次元を先頭に追加する\n",
        "        \"\"\"\n",
        "        pos_v = np.zeros(shape=[pos, embd_dim])\n",
        "        for p in range(pos):\n",
        "            for i in range(embd_dim):\n",
        "                if i % 2 == 0:\n",
        "                    pos_v[p,i] = np.sin(p / np.power(10000, (i / embd_dim)))\n",
        "                else:\n",
        "                    pos_v[p,i] = np.cos(p / np.power(10000, ((i - 1) / embd_dim)))\n",
        "        return pos_v[None,...]\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pos_vec = self.positional_vec(input_shape[1], input_shape[-1])\n",
        "        self.pos_vec = tf.constant(pos_vec, dtype=tf.float32)\n",
        "        super().build(input_shape)\n",
        "        \n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: embeddingされた文章のtensor (batch_size, max_length, hidden_dim)\n",
        "        Returns:\n",
        "            inputに位置ベクトルを加算したtensor (batch_size, max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        return tf.add(input, self.pos_vec)"
      ],
      "metadata": {
        "id": "P94C7y6nIuW4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer"
      ],
      "metadata": {
        "id": "MdBcEVMt5Qc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    一層のEncoder\n",
        "    Attributes:\n",
        "        at_weight_dim: attention機構で使用する重みの次元 \n",
        "        num_heads: multi head attentionのhead数\n",
        "        ffn_weight_dim: 全結合層の重みの次元。embeddingの次元に一致させる\n",
        "        dropout_rate: dropout層のパラメータ\n",
        "    \"\"\" \n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        at_weight_dim=512, \n",
        "        num_heads=8,\n",
        "        ffn_weight_dim=256, \n",
        "        dropout_rate=0.2,\n",
        "        **kwargs\n",
        "        ):\n",
        "\n",
        "        super().__init__(**kwargs) \n",
        "        self.at_weight_dim = at_weight_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ffn_weight_dim = ffn_weight_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.self_attention = EncoderSelfAttention(\n",
        "            self.at_weight_dim, self.num_heads)\n",
        "        self.layer_norm1 = LayerNormalizer()\n",
        "        self.layer_norm2 = LayerNormalizer()\n",
        "        self.ffn = FeedForwardNeuralBlock(self.ffn_weight_dim, self.dropout_rate)\n",
        "\n",
        "        \n",
        "    def call(self, input, attention_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: tensor (batch_size, max_length, hidden_dim)\n",
        "            attention_mask: np.array (batch_size, max_length)\n",
        "        Returns:\n",
        "            tensor (batch_size, max_length, hidden_dim)\n",
        "        \"\"\"    \n",
        "        out1 = self.self_attention(input, attention_mask)\n",
        "        out1 = self.layer_norm1(input + out1)\n",
        "\n",
        "        out2 = self.ffn(out1)\n",
        "        out2 = self.layer_norm2(out1 + out2)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    一層のDecoder\n",
        "    Attributes:\n",
        "        at_weight_dim: attention機構で使用する重みの次元 \n",
        "        num_heads: multi head attentionのhead数\n",
        "        ffn_weight_dim: 全結合層の重みの次元。embeddingの次元に一致させる\n",
        "        dropout_rate: dropout層のパラメータ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,  \n",
        "        at_weight_dim=512, \n",
        "        num_heads=8,\n",
        "        ffn_weight_dim=256,\n",
        "        dropout_rate=0.2,\n",
        "        **kwargs\n",
        "        ):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        self.at_weight_dim = at_weight_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ffn_weight_dim = ffn_weight_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.self_attention = DecoderSelfAttention(\n",
        "            self.at_weight_dim, self.num_heads)\n",
        "        self.ed_attention = EncoderDecoderAttention(\n",
        "            self.at_weight_dim, self.num_heads)\n",
        "        self.ffn = FeedForwardNeuralBlock(self.ffn_weight_dim, self.dropout_rate)\n",
        "        self.layer_norm1 = LayerNormalizer()\n",
        "        self.layer_norm2 = LayerNormalizer()\n",
        "        self.layer_norm3 = LayerNormalizer()\n",
        "\n",
        "\n",
        "    def call(self, \n",
        "             decoder_input, \n",
        "             decoder_attention_mask, \n",
        "             encoder_output,\n",
        "             encoder_attention_mask\n",
        "             ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_input: decoder側の入力tensor (batch_size, decoder_max_length, hidden_dim)\n",
        "            decoder_attention_mask: np.array (batch_size, decoder_max_length)\n",
        "            encoder_output: encoder側の最終出力tensor (batch_size, encoder_max_length, hidden_dim)\n",
        "            encoder_attention_mask: np.array (batch_size, encoder_max_length)\n",
        "        \n",
        "        Returns:\n",
        "            tensor (batch_size, decoder_max_length, hidden_dim)\n",
        "        \"\"\"\n",
        "        \n",
        "        out1 = self.self_attention(decoder_input, decoder_attention_mask)\n",
        "        out1 = self.layer_norm1(decoder_input + out1)\n",
        "        \n",
        "        out2 = self.ed_attention(\n",
        "            out1, decoder_attention_mask, encoder_output, encoder_attention_mask)\n",
        "        out2 = self.layer_norm2(out1 + out2)\n",
        "\n",
        "        out3 = self.ffn(out2)\n",
        "        out3 = self.layer_norm3(out2 + out3)\n",
        "        return out3\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "        encoder_num_vocabs: encoder側の語彙数\n",
        "        decoder_num_vocabs: decoder側の語彙数\n",
        "        hidden_dim: embeddingベクトル及びEncoder,Decoder層の出力ベクトルの次元\n",
        "        at_weight_dim: attention機構で用いる重みの次元\n",
        "        num_heads: multi head attentionのhead数\n",
        "        dropout_rate: dropout層のパラメータ\n",
        "        num_encoders: Encoder層を積み上げる個数\n",
        "        num_decoders: Decoder層を積み上げる個数\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder_num_vocabs,\n",
        "                 decoder_num_vocabs,\n",
        "                 hidden_dim=256,\n",
        "                 at_weight_dim=512, \n",
        "                 num_heads=8,\n",
        "                 dropout_rate=0.2, \n",
        "                 num_encoders=8,\n",
        "                 num_decoders=8,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder_num_vocabs = encoder_num_vocabs\n",
        "        self.decoder_num_vocabs = decoder_num_vocabs\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.at_weight_dim = at_weight_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_encoders = num_encoders\n",
        "        self.num_decoders = num_decoders\n",
        "\n",
        "        self.encoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "            encoder_num_vocabs, hidden_dim)\n",
        "        self.decoder_embedding_layer = tf.keras.layers.Embedding(\n",
        "            decoder_num_vocabs, hidden_dim)\n",
        "        self.encoder_pe_layer = PositionalEncoder()\n",
        "        self.decoder_pe_layer = PositionalEncoder()\n",
        "\n",
        "        \n",
        "        self.encoders_list = []\n",
        "        self.decoders_list = []\n",
        "\n",
        "        for _ in range(self.num_encoders):\n",
        "            self.encoders_list.append(\n",
        "                Encoder(at_weight_dim=at_weight_dim,\n",
        "                        num_heads=num_heads,\n",
        "                        ffn_weight_dim=hidden_dim,\n",
        "                        dropout_rate=dropout_rate)\n",
        "                )\n",
        "            \n",
        "        for _ in range(self.num_decoders):\n",
        "            self.decoders_list.append(\n",
        "                Decoder(at_weight_dim=at_weight_dim,\n",
        "                        num_heads=num_heads,\n",
        "                        ffn_weight_dim=hidden_dim,\n",
        "                        dropout_rate=dropout_rate)\n",
        "                )\n",
        "            \n",
        "        self.vocab_prob_layer = tf.keras.layers.Dense(\n",
        "            decoder_num_vocabs, name=\"vocab_prob_layer\", activation=\"softmax\")\n",
        "        \n",
        "            \n",
        "    def call(self, \n",
        "             encoder_input_ids, \n",
        "             encoder_attention_mask,\n",
        "             decoder_input_ids,\n",
        "             decoder_attention_mask\n",
        "             ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder_input_ids: encoder側の入力token id np.array (batch_size, encoder_max_length)\n",
        "            encoder_attention_mask: np.array (batch_size, encoder_max_length)\n",
        "            decoder_input: decoder側の入力token id np.array (batch_size, decoder_max_length)\n",
        "            decoder_attention_mask: np.array (batch_size, decoder_max_length)\n",
        "        \n",
        "        Returns:\n",
        "            tensor (batch_size, decoder_max_length, hidden_dim)       \n",
        "        \"\"\"\n",
        "        encoder_vec = self.encoder_embedding_layer(encoder_input_ids)\n",
        "        encoder_vec = self.encoder_pe_layer(encoder_vec)\n",
        "\n",
        "        for encoder in self.encoders_list:\n",
        "            encoder_vec = encoder(encoder_vec, encoder_attention_mask)\n",
        "\n",
        "        decoder_vec = self.decoder_embedding_layer(decoder_input_ids)\n",
        "        decoder_vec = self.decoder_pe_layer(decoder_vec)\n",
        "\n",
        "        for decoder in self.decoders_list:\n",
        "            decoder_vec = decoder(\n",
        "                decoder_vec, decoder_attention_mask, encoder_vec, encoder_attention_mask)\n",
        "        \n",
        "        vocab_prob = self.vocab_prob_layer(decoder_vec)\n",
        "        \n",
        "        return {\"vocab_prob\": vocab_prob, \"last_hidden_state\": decoder_vec}"
      ],
      "metadata": {
        "id": "1-NkCKow3tsq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build model"
      ],
      "metadata": {
        "id": "x8ZZL2YR5eR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "\n",
        "    encoder_input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"encoder_input_ids\"\n",
        "        )\n",
        "    encoder_attention_mask = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"encoder_attention_mask\"\n",
        "        )\n",
        "    decoder_input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"decoder_input_ids\"\n",
        "        ) \n",
        "    decoder_attention_mask = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"decoder_attention_mask\"\n",
        "        )\n",
        "    \n",
        "    transformer = Transformer(\n",
        "                 encoder_num_vocabs=np.unique(encoder_token_list).shape[0],\n",
        "                 decoder_num_vocabs=np.unique(decoder_token_list).shape[0],\n",
        "                 hidden_dim=64,\n",
        "                 at_weight_dim=128, \n",
        "                 num_heads=4,\n",
        "                 dropout_rate=0.2, \n",
        "                 num_encoders=4,\n",
        "                 num_decoders=4,\n",
        "                 )\n",
        "    \n",
        "    output = transformer(\n",
        "        encoder_input_ids, \n",
        "        decoder_input_ids, \n",
        "        encoder_attention_mask,\n",
        "        decoder_attention_mask\n",
        "        )\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[encoder_input_ids, \n",
        "                                   encoder_attention_mask,\n",
        "                                   decoder_input_ids,\n",
        "                                   decoder_attention_mask],\n",
        "                           outputs=output)\n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=[\"acc\"])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "YyvAm1Ow3xKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zQkcEn1e_Hu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}